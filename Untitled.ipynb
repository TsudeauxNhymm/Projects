{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-446de94cee4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english-german.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-446de94cee4c>\u001b[0m in \u001b[0;36mload_clean_sentences\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_clean_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "from numpy.random import rand\n",
    "\n",
    "from numpy.random import shuffle\n",
    "\n",
    "import re\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "from unicodedata import normalize\n",
    "\n",
    "import string\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\n",
    "\t# open the file as read only\n",
    "\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\n",
    "\t# read all text\n",
    "\n",
    "\ttext = file.read()\n",
    "\n",
    "\t# close the file\n",
    "\n",
    "\tfile.close()\n",
    "\n",
    "\treturn text\n",
    "\n",
    " \n",
    "\n",
    "# split a loaded document into sentences\n",
    "\n",
    "def to_pairs(doc):\n",
    "\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\n",
    "\treturn pairs\n",
    "\n",
    " \n",
    "\n",
    "# clean a list of lines\n",
    "\n",
    "def clean_pairs(lines):\n",
    "\n",
    "\tcleaned = list()\n",
    "\n",
    "\t# prepare regex for char filtering\n",
    "\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\n",
    "\t# prepare translation table for removing punctuation\n",
    "\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\tfor pair in lines:\n",
    "\n",
    "\t\tclean_pair = list()\n",
    "\n",
    "\t\tfor line in pair:\n",
    "\n",
    "\t\t\t# normalize unicode characters\n",
    "\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\n",
    "\t\t\t# tokenize on white space\n",
    "\n",
    "\t\t\tline = line.split()\n",
    "\n",
    "\t\t\t# convert to lowercase\n",
    "\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\n",
    "\t\t\t# remove punctuation from each token\n",
    "\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\n",
    "\t\t\t# store as string\n",
    "\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\n",
    "\treturn array(cleaned)\n",
    "\n",
    " \n",
    "\n",
    "# load a clean dataset\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    " \n",
    "\n",
    "# save a list of clean sentences to file\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    " \n",
    "\n",
    "# load dataset\n",
    "\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    " \n",
    "\n",
    "# reduce dataset size\n",
    "\n",
    "n_sentences = 10000\n",
    "\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "\n",
    "shuffle(dataset)\n",
    "\n",
    "# split into train/test\n",
    "\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "# save\n",
    "\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "\n",
    "save_clean_data(test, 'english-german-test.pkl')\n",
    " \n",
    "\n",
    "# load a clean dataset\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    " \n",
    "\n",
    "# fit a tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "\n",
    "\ttokenizer = Tokenizer()\n",
    "\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\n",
    "\treturn tokenizer\n",
    "\n",
    " \n",
    "\n",
    "# max sentence length\n",
    "\n",
    "def max_length(lines):\n",
    "\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    " \n",
    "\n",
    "# encode and pad sequences\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\n",
    "\t# integer encode sequences\n",
    "\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "\t# pad sequences with 0 values\n",
    "\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\n",
    "\treturn X\n",
    "\n",
    " \n",
    "\n",
    "# one hot encode target sequence\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "\n",
    "\tylist = list()\n",
    "\n",
    "\tfor sequence in sequences:\n",
    "\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\n",
    "\t\tylist.append(encoded)\n",
    "\n",
    "\ty = array(ylist)\n",
    "\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\n",
    "\treturn y\n",
    "\n",
    " \n",
    "\n",
    "# define NMT model\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\n",
    "\treturn model\n",
    "\n",
    " \n",
    "\n",
    "# load datasets\n",
    "\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    " \n",
    "\n",
    "# prepare english tokenizer\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    " \n",
    "\n",
    "# prepare training data\n",
    "\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    " \n",
    "\n",
    "# define model\n",
    "\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# summarize defined model\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "\n",
    "filename = 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
